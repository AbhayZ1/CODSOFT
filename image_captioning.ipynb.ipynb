{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.15","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"tpu1vmV38","dataSources":[],"dockerImageVersionId":30805,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"pip install --upgrade tensorflow","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-19T14:20:14.181326Z","iopub.execute_input":"2025-01-19T14:20:14.181758Z","iopub.status.idle":"2025-01-19T14:21:14.956510Z","shell.execute_reply.started":"2025-01-19T14:20:14.181723Z","shell.execute_reply":"2025-01-19T14:21:14.955364Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/site-packages (2.16.1)\nCollecting tensorflow\n  Downloading tensorflow-2.18.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (615.3 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m615.3/615.3 MB\u001b[0m \u001b[31m771.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/site-packages (from tensorflow) (4.12.2)\nRequirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/site-packages (from tensorflow) (1.6.3)\nRequirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/site-packages (from tensorflow) (3.20.3)\nRequirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/site-packages (from tensorflow) (1.16.0)\nRequirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.10/site-packages (from tensorflow) (24.3.25)\nRequirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/site-packages (from tensorflow) (3.4.0)\nRequirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/site-packages (from tensorflow) (2.5.0)\nRequirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.10/site-packages (from tensorflow) (3.12.1)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.10/site-packages (from tensorflow) (65.5.1)\nRequirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/site-packages (from tensorflow) (0.37.1)\nCollecting ml-dtypes<0.5.0,>=0.4.0\n  Downloading ml_dtypes-0.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.2 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m61.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/site-packages (from tensorflow) (1.68.0)\nRequirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/site-packages (from tensorflow) (1.17.0)\nRequirement already satisfied: numpy<2.1.0,>=1.26.0 in /usr/local/lib/python3.10/site-packages (from tensorflow) (1.26.4)\nRequirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/site-packages (from tensorflow) (2.1.0)\nRequirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/site-packages (from tensorflow) (0.2.0)\nCollecting tensorboard<2.19,>=2.18\n  Downloading tensorboard-2.18.0-py3-none-any.whl (5.5 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m76.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/site-packages (from tensorflow) (24.2)\nRequirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.10/site-packages (from tensorflow) (3.6.0)\nRequirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/site-packages (from tensorflow) (0.6.0)\nRequirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/site-packages (from tensorflow) (18.1.1)\nRequirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/site-packages (from tensorflow) (2.32.3)\nRequirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/site-packages (from astunparse>=1.6.0->tensorflow) (0.45.0)\nRequirement already satisfied: namex in /usr/local/lib/python3.10/site-packages (from keras>=3.5.0->tensorflow) (0.0.8)\nRequirement already satisfied: optree in /usr/local/lib/python3.10/site-packages (from keras>=3.5.0->tensorflow) (0.13.1)\nRequirement already satisfied: rich in /usr/local/lib/python3.10/site-packages (from keras>=3.5.0->tensorflow) (13.9.4)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow) (2024.8.30)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow) (3.4.0)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow) (2.2.3)\nRequirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/site-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.7)\nRequirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/site-packages (from tensorboard<2.19,>=2.18->tensorflow) (0.7.2)\nRequirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/site-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.1.3)\nRequirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow) (3.0.2)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/site-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/site-packages (from rich->keras>=3.5.0->tensorflow) (2.18.0)\nRequirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\nInstalling collected packages: ml-dtypes, tensorboard, tensorflow\n  Attempting uninstall: ml-dtypes\n    Found existing installation: ml-dtypes 0.3.2\n    Uninstalling ml-dtypes-0.3.2:\n      Successfully uninstalled ml-dtypes-0.3.2\n  Attempting uninstall: tensorboard\n    Found existing installation: tensorboard 2.16.2\n    Uninstalling tensorboard-2.16.2:\n      Successfully uninstalled tensorboard-2.16.2\n  Attempting uninstall: tensorflow\n    Found existing installation: tensorflow 2.16.1\n    Uninstalling tensorflow-2.16.1:\n      Successfully uninstalled tensorflow-2.16.1\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ntf-keras 2.16.0 requires tensorflow<2.17,>=2.16, but you have tensorflow 2.18.0 which is incompatible.\ntensorflow-text 2.16.1 requires tensorflow<2.17,>=2.16.1; platform_machine != \"arm64\" or platform_system != \"Darwin\", but you have tensorflow 2.18.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed ml-dtypes-0.4.1 tensorboard-2.18.0 tensorflow-2.18.0\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"import numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Input, Embedding, LSTM, Dense, Dropout, Add, GlobalAveragePooling2D\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.datasets import cifar10\nfrom tensorflow.keras.applications import MobileNetV2\n\n# Load and prepare the CIFAR-10 dataset\n(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n\n# CIFAR-10 classes\nclass_names = [\n    \"airplane\", \"automobile\", \"bird\", \"cat\", \"deer\",\n    \"dog\", \"frog\", \"horse\", \"ship\", \"truck\"\n]\n\n# Prepare captions (limit to 5000 samples)\nnum_samples = 5000\ncaptions = [f\"This is a {class_names[label[0]]}\" for label in y_train[:num_samples]]\n\n# Tokenize captions\ntokenizer = Tokenizer(num_words=5000, oov_token=\"<unk>\")\ntokenizer.fit_on_texts(captions)\nsequences = tokenizer.texts_to_sequences(captions)\nmax_len = max([len(seq) for seq in sequences])\npadded_captions = pad_sequences(sequences, maxlen=max_len, padding='post')\n\n# Prepare target data\ny_train_target = np.array(sequences)[:, 1:]  # Remove first word (start token)\ny_train_target = pad_sequences(y_train_target, maxlen=max_len-1, padding=\"post\")\n\nclass ImageCaptioningModel(tf.keras.Model):\n    def __init__(self, vocab_size, max_len, **kwargs):\n        super(ImageCaptioningModel, self).__init__(**kwargs)\n        \n        # Image processing layers\n        self.base_model = MobileNetV2(\n            input_shape=(96, 96, 3),\n            include_top=False,\n            weights='imagenet'\n        )\n        self.base_model.trainable = False\n        \n        self.global_pool = GlobalAveragePooling2D()\n        self.image_dense = Dense(256, activation='relu')\n        self.image_dropout = Dropout(0.3)\n        self.repeat_vector = tf.keras.layers.RepeatVector(max_len-1)\n        \n        # Text processing layers\n        self.embedding = Embedding(vocab_size, 256, mask_zero=True)\n        self.text_lstm = LSTM(256, return_sequences=True, dropout=0.3)\n        \n        # Decoder layers\n        self.decoder_lstm = LSTM(512, return_sequences=True, dropout=0.3)\n        self.decoder_dense1 = Dense(512, activation='relu')\n        self.decoder_dropout = Dropout(0.3)\n        self.output_dense = Dense(vocab_size, activation='softmax')\n        \n    def call(self, inputs):\n        image_input, text_input = inputs\n        \n        # Process image\n        x_img = self.base_model(image_input)\n        x_img = self.global_pool(x_img)\n        x_img = self.image_dense(x_img)\n        x_img = self.image_dropout(x_img)\n        x_img = self.repeat_vector(x_img)\n        \n        # Process text\n        x_text = self.embedding(text_input)\n        x_text = self.text_lstm(x_text)\n        x_text = x_text[:, :-1, :]  # Remove last timestep\n        \n        # Combine features\n        decoder_inputs = x_img + x_text\n        \n        # Decode\n        x = self.decoder_lstm(decoder_inputs)\n        x = self.decoder_dense1(x)\n        x = self.decoder_dropout(x)\n        outputs = self.output_dense(x)\n        \n        return outputs\n\n# Prepare the data\nprint(\"Preparing training data...\")\nx_train_resized = tf.image.resize(x_train[:num_samples], (96, 96))\nx_train_resized = tf.cast(x_train_resized, tf.float32) / 255.0\n\n# Create and compile model\nprint(\"Creating model...\")\nvocab_size = len(tokenizer.word_index) + 1\nmodel = ImageCaptioningModel(vocab_size, max_len)\n\n# Use a lower learning rate for stability\noptimizer = tf.keras.optimizers.Adam(learning_rate=0.0001)\n\nmodel.compile(\n    optimizer=optimizer,\n    loss='sparse_categorical_crossentropy',\n    metrics=['accuracy']\n)\n\n# Prepare target data for training\ny_train_target_sparse = np.expand_dims(y_train_target, axis=-1)\n\n# Create tf.data.Dataset\ntrain_dataset = tf.data.Dataset.from_tensor_slices((\n    {\n        \"image_input\": x_train_resized,\n        \"text_input\": padded_captions\n    },\n    y_train_target_sparse\n))\n\n# Shuffle and batch the dataset\ntrain_dataset = train_dataset.shuffle(1000).batch(16)\n\n# Train the model\nprint(\"Training model...\")\nimport tensorflow as tf\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\n\n# Assuming `x_train_resized` and `y_train_target_sparse` are already NumPy arrays\nx_train_resized_np = x_train_resized  # Use the existing NumPy array\ny_train_target_np = y_train_target_sparse  # Use the existing NumPy array\n\n# Split into training and validation sets\nx_train, x_val, y_train, y_val = train_test_split(x_train_resized_np, y_train_target_np, test_size=0.2)\n\n# Ensure the data is in the correct shape\n# The x_train_resized and y_train_target arrays should be in the shape: (batch_size, height, width, channels)\n# and (batch_size, seq_length), respectively\n\n# Create tf.data.Dataset\ntrain_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(64)\nval_dataset = tf.data.Dataset.from_tensor_slices((x_val, y_val)).batch(64)\n\n# Assuming `model` is already defined and compiled\nhistory = model.fit(\n    train_dataset,  # Training data\n    epochs=10,      # Number of epochs\n    validation_data=val_dataset,  # Validation data\n    batch_size=64   # Batch size\n)\n\n\n# Save the model\nmodel.save_weights(\"captioning_model_mobilenetv2_weights.h5\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-01-19T14:38:55.026933Z","iopub.execute_input":"2025-01-19T14:38:55.027333Z","iopub.status.idle":"2025-01-19T14:38:59.157826Z","shell.execute_reply.started":"2025-01-19T14:38:55.027303Z","shell.execute_reply":"2025-01-19T14:38:59.156479Z"}},"outputs":[{"name":"stdout","text":"Preparing training data...\nCreating model...\nTraining model...\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[26], line 132\u001b[0m\n\u001b[1;32m    129\u001b[0m y_train_target_np \u001b[38;5;241m=\u001b[39m y_train_target_sparse  \u001b[38;5;66;03m# Use the existing NumPy array\u001b[39;00m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;66;03m# Split into training and validation sets\u001b[39;00m\n\u001b[0;32m--> 132\u001b[0m x_train, x_val, y_train, y_val \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_test_split\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_train_resized_np\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train_target_np\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;66;03m# Ensure the data is in the correct shape\u001b[39;00m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;66;03m# The x_train_resized and y_train_target arrays should be in the shape: (batch_size, height, width, channels)\u001b[39;00m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;66;03m# and (batch_size, seq_length), respectively\u001b[39;00m\n\u001b[1;32m    137\u001b[0m \n\u001b[1;32m    138\u001b[0m \u001b[38;5;66;03m# Create tf.data.Dataset\u001b[39;00m\n\u001b[1;32m    139\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataset\u001b[38;5;241m.\u001b[39mfrom_tensor_slices((x_train, y_train))\u001b[38;5;241m.\u001b[39mbatch(\u001b[38;5;241m64\u001b[39m)\n","File \u001b[0;32m/usr/local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py:213\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    208\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m    209\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m    210\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m    211\u001b[0m         )\n\u001b[1;32m    212\u001b[0m     ):\n\u001b[0;32m--> 213\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    215\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[1;32m    217\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[1;32m    219\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[1;32m    220\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    221\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    222\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[1;32m    223\u001b[0m     )\n","File \u001b[0;32m/usr/local/lib/python3.10/site-packages/sklearn/model_selection/_split.py:2810\u001b[0m, in \u001b[0;36mtrain_test_split\u001b[0;34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001b[0m\n\u001b[1;32m   2806\u001b[0m     train, test \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(cv\u001b[38;5;241m.\u001b[39msplit(X\u001b[38;5;241m=\u001b[39marrays[\u001b[38;5;241m0\u001b[39m], y\u001b[38;5;241m=\u001b[39mstratify))\n\u001b[1;32m   2808\u001b[0m train, test \u001b[38;5;241m=\u001b[39m ensure_common_namespace_device(arrays[\u001b[38;5;241m0\u001b[39m], train, test)\n\u001b[0;32m-> 2810\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2811\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_iterable\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2812\u001b[0m \u001b[43m        \u001b[49m\u001b[43m(\u001b[49m\u001b[43m_safe_indexing\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_safe_indexing\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43ma\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43marrays\u001b[49m\n\u001b[1;32m   2813\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2814\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/usr/local/lib/python3.10/site-packages/sklearn/model_selection/_split.py:2812\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   2806\u001b[0m     train, test \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(cv\u001b[38;5;241m.\u001b[39msplit(X\u001b[38;5;241m=\u001b[39marrays[\u001b[38;5;241m0\u001b[39m], y\u001b[38;5;241m=\u001b[39mstratify))\n\u001b[1;32m   2808\u001b[0m train, test \u001b[38;5;241m=\u001b[39m ensure_common_namespace_device(arrays[\u001b[38;5;241m0\u001b[39m], train, test)\n\u001b[1;32m   2810\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(\n\u001b[1;32m   2811\u001b[0m     chain\u001b[38;5;241m.\u001b[39mfrom_iterable(\n\u001b[0;32m-> 2812\u001b[0m         (\u001b[43m_safe_indexing\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m)\u001b[49m, _safe_indexing(a, test)) \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m arrays\n\u001b[1;32m   2813\u001b[0m     )\n\u001b[1;32m   2814\u001b[0m )\n","File \u001b[0;32m/usr/local/lib/python3.10/site-packages/sklearn/utils/_indexing.py:267\u001b[0m, in \u001b[0;36m_safe_indexing\u001b[0;34m(X, indices, axis)\u001b[0m\n\u001b[1;32m    265\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _polars_indexing(X, indices, indices_dtype, axis\u001b[38;5;241m=\u001b[39maxis)\n\u001b[1;32m    266\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(X, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshape\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 267\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_array_indexing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindices_dtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    269\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _list_indexing(X, indices, indices_dtype)\n","File \u001b[0;32m/usr/local/lib/python3.10/site-packages/sklearn/utils/_indexing.py:33\u001b[0m, in \u001b[0;36m_array_indexing\u001b[0;34m(array, key, key_dtype, axis)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m     32\u001b[0m     key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[0;32m---> 33\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43marray\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m axis \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m array[:, key]\n","File \u001b[0;32m/usr/local/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m--> 153\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n","File \u001b[0;32m/usr/local/lib/python3.10/site-packages/tensorflow/python/ops/tensor_getitem_override.py:62\u001b[0m, in \u001b[0;36m_check_index\u001b[0;34m(idx)\u001b[0m\n\u001b[1;32m     57\u001b[0m dtype \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(idx, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m dtypes\u001b[38;5;241m.\u001b[39mas_dtype(dtype) \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m _SUPPORTED_SLICE_DTYPES \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[1;32m     59\u001b[0m     idx\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(idx\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m     60\u001b[0m   \u001b[38;5;66;03m# TODO(slebedev): IndexError seems more appropriate here, but it\u001b[39;00m\n\u001b[1;32m     61\u001b[0m   \u001b[38;5;66;03m# will break `_slice_helper` contract.\u001b[39;00m\n\u001b[0;32m---> 62\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(_SLICE_TYPE_ERROR \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, got \u001b[39m\u001b[38;5;132;01m{!r}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(idx))\n","\u001b[0;31mTypeError\u001b[0m: Only integers, slices (`:`), ellipsis (`...`), tf.newaxis (`None`) and scalar tf.int32/tf.int64 tensors are valid indices, got array([   5, 2074, 4891, ..., 4031, 2156, 3208])"],"ename":"TypeError","evalue":"Only integers, slices (`:`), ellipsis (`...`), tf.newaxis (`None`) and scalar tf.int32/tf.int64 tensors are valid indices, got array([   5, 2074, 4891, ..., 4031, 2156, 3208])","output_type":"error"}],"execution_count":26},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}